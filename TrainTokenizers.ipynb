{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aktarafder/Documents/SoftwareProject/HF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 8.44k/8.44k [00:00<00:00, 6.81MB/s]\n",
      "Downloading metadata: 100%|██████████| 18.5k/18.5k [00:00<00:00, 10.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset code_search_net/java to /home/aktarafder/.cache/huggingface/datasets/code_search_net/java/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.06G/1.06G [00:10<00:00, 103MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:12<00:00, 12.98s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:26<00:00, 26.85s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:13<00:00,  4.52s/it]\n",
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset code_search_net downloaded and prepared to /home/aktarafder/.cache/huggingface/datasets/code_search_net/java/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import transformers\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, Split\n",
    "import json\n",
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names,get_dataset_config_names\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import BertConfig, BertLMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# loading codesearch net java dataset\n",
    "dataset_codeSearch_java=load_dataset(\"code_search_net\",\"java\")\n",
    "djava=dataset_codeSearch_java\n",
    "print(\"loaded the dataset.\")\n",
    "unk_token = \"<UNK>\"  # token for unknown words\n",
    "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_tokenizer_trainer(alg='byteBPE'):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if alg=='byteBPE':\n",
    "        tokenizer = ByteLevelBPETokenizer()\n",
    "    \n",
    "    #tokenizer.pre_tokenizer = Split(';','removed')\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer=prepare_tokenizer_trainer('byteBPE')\n",
    "tokenizer.train(files='./tokens1to25k_java_NOAST.txt',vocab_size=50000, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\"])\n",
    "\n",
    "tokenizer.save('./ASTbpeTokens1to25k_no_Ast_java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_ast=prepare_tokenizer_trainer('byteBPE')\n",
    "tokenizer_ast.train(files='./tokens1to25k_java_ast.txt',vocab_size=50000, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\"])\n",
    "\n",
    "tokenizer.save('./ASTbpeTokens1to25k_Ast_java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
